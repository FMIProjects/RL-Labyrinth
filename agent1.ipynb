{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent 1 - SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T18:54:31.869524Z",
     "start_time": "2024-12-27T18:54:31.591905Z"
    }
   },
   "source": [
    "from agents.SARSA_agent import SarsaAgent\n",
    "from agents.base_agent import test_agent\n",
    "from environment.maze_env import MazeEnv"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.30.7, Python 3.10.15)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:241: RuntimeWarning: Your system is avx2 capable but pygame was not built with support for it. The performance of some of your blits could be adversely affected. Consider enabling compile time detection with environment variables like PYGAME_DETECT_AVX2=1 if you are compiling without cross compilation.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Env + Agent Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T18:54:33.515923Z",
     "start_time": "2024-12-27T18:54:33.501214Z"
    }
   },
   "source": [
    "env = MazeEnv(\n",
    "    width=6,\n",
    "    height=6,\n",
    "    num_keys=3,\n",
    "    num_obstacles=0,\n",
    "    peek_distance=1,\n",
    "    distance_type=\"manhattan\",\n",
    ")\n",
    "\n",
    "agent = SarsaAgent(\n",
    "    env=env,\n",
    "    alpha=0.1,\n",
    "    gamma=0.99,\n",
    "    epsilon=1.0,\n",
    "    epsilon_decay=0.995,\n",
    "    epsilon_min=0.1,\n",
    "    load_pickle_path=None,\n",
    "    store_pickle_path=\"./agent_rack/sarsa_agent.pkl\",\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Train"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T18:55:00.340266Z",
     "start_time": "2024-12-27T18:54:35.048277Z"
    }
   },
   "source": [
    "print(\"Training started...\")\n",
    "agent.train(episodes=100000)\n",
    "agent.serialize()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1172/100000 [00:25<35:24, 46.51it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTraining started...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 2\u001B[0m \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepisodes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100000\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m agent\u001B[38;5;241m.\u001B[39mserialize()\n",
      "File \u001B[0;32m~/PycharmProjects/RL-Labyrinth/agents/SARSA_agent.py:84\u001B[0m, in \u001B[0;36mSarsaAgent.train\u001B[0;34m(self, episodes)\u001B[0m\n\u001B[1;32m     80\u001B[0m done \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m     82\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m done:\n\u001B[1;32m     83\u001B[0m     \u001B[38;5;66;03m# Get the next state reward and choose the next action\u001B[39;00m\n\u001B[0;32m---> 84\u001B[0m     next_state, reward, done, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcurrent_action\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     85\u001B[0m     next_action \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchoose_action(next_state)\n\u001B[1;32m     87\u001B[0m     \u001B[38;5;66;03m# Update the q value\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/RL-Labyrinth/environment/maze_env.py:304\u001B[0m, in \u001B[0;36mMazeEnv.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    301\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_keys_distances()\n\u001B[1;32m    303\u001B[0m \u001B[38;5;66;03m# Compute peek maze\u001B[39;00m\n\u001B[0;32m--> 304\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_peek_maze\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    306\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_observation(), reward, done, {}\n",
      "File \u001B[0;32m~/PycharmProjects/RL-Labyrinth/environment/maze_env.py:356\u001B[0m, in \u001B[0;36mMazeEnv.compute_peek_maze\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    353\u001B[0m offset_col \u001B[38;5;241m=\u001B[39m col \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpeek_distance\n\u001B[1;32m    355\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(row \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpeek_distance, row \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpeek_distance \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m--> 356\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m j \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28;43mrange\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcol\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpeek_distance\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcol\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpeek_distance\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m:\n\u001B[1;32m    358\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;241m0\u001B[39m \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m i \u001B[38;5;241m<\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mheight \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;241m0\u001B[39m \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m j \u001B[38;5;241m<\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwidth:\n\u001B[1;32m    359\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpeek_maze[i \u001B[38;5;241m-\u001B[39m offset_row, j \u001B[38;5;241m-\u001B[39m offset_col] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmaze[i, j]\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Test"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Testing started...\")\n",
    "test_agent(env, agent, episodes=1)\n",
    "\n",
    "    def update_q_value(self, state, action, reward):\n",
    "        \"\"\"\n",
    "        Update a Q-value entry using Q Learning formula.\n",
    "        Q(s,a) = Q(s,a) + α * (reward + γ * max a ( Q(s',a) ) - Q(s,a))\n",
    "        \"\"\"\n",
    "\n",
    "        current_q = self.q_table.get((state, action), 0)\n",
    "\n",
    "        max_q = max(\n",
    "            range(self.env.action_space.n),\n",
    "            key=lambda a: self.q_table.get((state, a), 0),\n",
    "        )\n",
    "\n",
    "        temporal_difference_value = reward + self.gamma * max_q\n",
    "        temporal_difference_error = temporal_difference_value - current_q\n",
    "        self.q_table[(state, action)] = (\n",
    "                current_q + self.alpha * temporal_difference_error\n",
    "        )"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL-Labyrinth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
