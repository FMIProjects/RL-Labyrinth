{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Agent - Sarsa\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T11:21:23.992805Z",
     "start_time": "2024-12-28T11:21:23.814673Z"
    }
   },
   "source": [
    "from agents.Q_learning_agent import QLearningAgent\n",
    "from agents.base_agent import test_agent\n",
    "from environment.maze_envs import MazeEnv"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.30.7, Python 3.10.15)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:241: RuntimeWarning: Your system is avx2 capable but pygame was not built with support for it. The performance of some of your blits could be adversely affected. Consider enabling compile time detection with environment variables like PYGAME_DETECT_AVX2=1 if you are compiling without cross compilation.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Env + Agent Setup\n",
    "\n",
    "- Medium size maze\n",
    "- No keys\n",
    "- No obstacles\n",
    "- Small observation space"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T11:22:00.590801Z",
     "start_time": "2024-12-28T11:22:00.580138Z"
    }
   },
   "source": [
    "\n",
    "# Try to load the agent otherwise create an untrained one\n",
    "agent = QLearningAgent(load_pickle_path=\"../agent_rack/q_learning_agent_medium.pkl\")\n",
    "try:\n",
    "    agent.deserialize()\n",
    "except Exception as e:\n",
    "\n",
    "    env = MazeEnv(\n",
    "        width=12,\n",
    "        height=12,\n",
    "        num_keys=0,\n",
    "        num_obstacles=0,\n",
    "        peek_distance=1,\n",
    "        distance_type=\"manhattan\",\n",
    "        new_layout_on_reset=False\n",
    "    )\n",
    "\n",
    "    agent = QLearningAgent(\n",
    "        env=env,\n",
    "        alpha=0.1,\n",
    "        gamma=0.99,\n",
    "        epsilon=1.0,\n",
    "        epsilon_decay=0.99993,\n",
    "        epsilon_min=0.1,\n",
    "        load_pickle_path=None,\n",
    "        store_pickle_path=\"../agent_rack/q_learning_agent_medium.pkl\",\n",
    ")\n",
    "\n",
    "print(f\"Agent q-table size: {len(agent.q_table)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent q-table size: 0\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Train"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T11:25:19.328330Z",
     "start_time": "2024-12-28T11:22:03.511172Z"
    }
   },
   "source": [
    "print(\"Training started...\")\n",
    "agent.train(episodes=200000)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 4270/200000 [03:15<2:29:29, 21.82it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTraining started...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 2\u001B[0m \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepisodes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m200000\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/RL-Labyrinth/agents/Q_learning_agent.py:90\u001B[0m, in \u001B[0;36mQLearningAgent.train\u001B[0;34m(self, episodes)\u001B[0m\n\u001B[1;32m     87\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m done:\n\u001B[1;32m     88\u001B[0m     \u001B[38;5;66;03m# Get the next state reward and choose the next action\u001B[39;00m\n\u001B[1;32m     89\u001B[0m     next_state, reward, done, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39mstep(current_action)\n\u001B[0;32m---> 90\u001B[0m     next_action \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchoose_action\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnext_state\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     92\u001B[0m     \u001B[38;5;66;03m# Update the q value\u001B[39;00m\n\u001B[1;32m     93\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mupdate_q_value(\n\u001B[1;32m     94\u001B[0m         current_state, current_action, reward,\n\u001B[1;32m     95\u001B[0m     )\n",
      "File \u001B[0;32m~/PycharmProjects/RL-Labyrinth/agents/Q_learning_agent.py:46\u001B[0m, in \u001B[0;36mQLearningAgent.choose_action\u001B[0;34m(self, state)\u001B[0m\n\u001B[1;32m     43\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39msample_action()\n\u001B[1;32m     45\u001B[0m \u001B[38;5;66;03m# Exploit the best path (return argmax(Q(s,a)))\u001B[39;00m\n\u001B[0;32m---> 46\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mmax\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m     47\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mrange\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maction_space\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     48\u001B[0m \u001B[43m    \u001B[49m\u001B[43mkey\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43ma\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mq_table\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43ma\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     49\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/RL-Labyrinth/agents/Q_learning_agent.py:48\u001B[0m, in \u001B[0;36mQLearningAgent.choose_action.<locals>.<lambda>\u001B[0;34m(a)\u001B[0m\n\u001B[1;32m     43\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39msample_action()\n\u001B[1;32m     45\u001B[0m \u001B[38;5;66;03m# Exploit the best path (return argmax(Q(s,a)))\u001B[39;00m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mmax\u001B[39m(\n\u001B[1;32m     47\u001B[0m     \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39maction_space\u001B[38;5;241m.\u001B[39mn),\n\u001B[0;32m---> 48\u001B[0m     key\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mlambda\u001B[39;00m a: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mq_table\u001B[38;5;241m.\u001B[39mget((state, a), \u001B[38;5;241m0\u001B[39m),\n\u001B[1;32m     49\u001B[0m )\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T11:06:04.249803Z",
     "start_time": "2024-12-28T11:06:04.204837Z"
    }
   },
   "cell_type": "code",
   "source": "agent.serialize()",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Test"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Testing started...\")\n",
    "test_agent(agent.env, agent, episodes=100,renderer_assets_dir_path=\"../assets\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL-Labyrinth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
